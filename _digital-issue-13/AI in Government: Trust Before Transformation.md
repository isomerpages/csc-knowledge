---
title: "AI in Government: Trust Before Transformation"
permalink: /ai-in-government-trust-before-transformation/
variant: markdown
description: ""
---
<style>


		
.back a
{
	color: #9f2943;
	font-weight: bold;
}

.break
{
   border-top: 1px solid  black;
   border-bottom: 1px solid black;
	 padding:20px;
	text-align:center;
	font-size:30px;
	margin-top:50px;
}
	
.break1
{
	font-family: Georgia;
	font-size:20px;
	font-style: italic;
	font-weight: bold;
}

	
.author
{
border-bottom: 1px solid black;
margin-top:40px;
padding-bottom:30px;
border-top: 1px solid black;
}
	
.author p
{
font-size: 15px;	
line-height: 22px;
}
	
.chatbot ol li
{
font-size: 16px;

}
	
.notestop ol li
{
font-size: 15px;
line-height:22px;
}		

#nutri img
{
width: 300px;	
}

.containerbox
{
padding: 20px;
background-color: #FFCC99;
}

.containerbox ol li
{
	font-size: 16px;
}

	
	
</style>
<em><small>ETHOS Digital Issue 13, Oct 2025</small></em>
<div class="background-image">
<img src="/images/Ethos_Images/Ethos_Digital_Issue_13/D13_AI_Banner_Resized.jpg">
</div>

<p>Artificial intelligence (AI) is reshaping our world at a pace and scale never-before-seen in human history. What was previously computationally impossible or expensive can become reality in mere seconds on personal devices. AI has the potential to empower millions, unlocking humanity's capacity to think, act, and dream in unprecedented ways.</p>

<p>Yet trust is required for this potential to materialise for government – trust that is elusive. A 2023 KPMG study found that three out of five (61%) people are wary about trusting AI systems and three quarters (73%) are concerned about its potential risks. Unfortunately, they also have the least confidence in the government's capacity to develop, utilise, and govern AI in the public's best interest, in comparison to other entities such as universities. </p>

<p>So how should government leaders engender trusted AI, and trust in AI? </p>

<h3>The Ethical AI-Powered Government</h3> 
<h4>AI-Empowered Service Delivery: User-Centred Control Of Data</h4>

<p>In the realm of service delivery, governments worldwide are transitioning from a multichannel model, characterised by fragmented databases and service touchpoints, to an integrated, omnichannel approach. In this new model, information flows seamlessly between databases, eliminating the need for users to repeat themselves. Instead of completing forms, users can engage with a chatbot, which can extract relevant information through voice chats, images, and document scans, and channel the data to the appropriate agencies directly. Aggregated data on users’ preferences and profile will enable AI to proactively recommend government services tailored to users' needs before they even initiate a request. </p>

<p>However, there are inherent concerns with creating these integrated, recommendation-based systems. One such concern is user control i.e., whether users will have the autonomy to manage how their information is shared.</p> 

<p>A positive case of AI deployment in this area is Finland’s AuroraAI, a recommendation-based system that suggests public and private services to citizens based on their significant life events. In developing the platform, the Finnish government prioritised participatory ethical design and established an Ethics Board to conduct a 360-risk evaluation. The board raised transparency and control as primary concerns. To address these, the team integrated AuroraAI with Finnish citizens' digital identity (DigiMe), granting users full oversight of their own database and complete autonomy over when, how, and with whom their data is shared. Finland's solution demonstrates how users’ autonomy can be honoured in the pursuit of more intelligent systems—an approach aligned to the high standards of data governance observed by healthcare systems.  
<sup><a href="#notestop">1</a></sup> </p>

<h4>AI-Simulated Policy and Decision-Making: Trust but Verify</h4>
<p>In the domain of policymaking, AI and digital twin technologies are enabling governments to better anticipate the impacts of their policies. A popular approach is through AI-powered agent-based modelling (ABM). ABM originally involves programming autonomous agents with simple 'rules' in a virtual environment to observe their behaviours and interactions with other agents and the environment. These simple rule-based agents can now be replaced by complex AI agents that closely emulate human behaviours, resulting in more accurate predictions.</p><p> 

</p><p>However, the danger of these simulations is their ability to project a veneer of objectivity and precision—when they are in fact simulacra of reality and not reality itself. </p><p>

</p><p>In 2020, Salesforce developed an ABM – dubbed the 'AI Economist'— to identify a tax policy that optimises the equality-productivity trade-off. Using worker and policymaker AI agents—each with pre-determined goals and varying skill levels that can learn, strategise, and co-adapt flexibly in the virtual environment—they discovered a tax scheme that reduced the trade-off between equality and productivity by 16%, surpassing existing known standards. Impressively, the tax scheme also demonstrated resilience against tax gaming and evasion strategies. </p><p>

</p><p>Nonetheless, recognising the potential risks of such simulations, Salesforce commissioned an ethical and human rights impact assessment<sup><a href="#notestop">2</a></sup> and took concrete steps to ensure the risks were mitigated. For example, they validated the tax model through 125 in-person games with 100+ US-based participants (and found that the outcomes were aligned to what the simulations had produced). To prevent users misusing the software, they only shared the simulation code with selected users who identified themselves, and clearly detailed the intended purpose, variables, assumptions, use cases and limitations of the model in a simulation card. Salesforce's cautious approach towards the deployment of their model underscores the importance of rigorous validation, transparency, and human oversight in dealing with AI-generated responses.</p> 

<h4>AI-Generated Datasets for Training and Analysis: Protecting Privacy</h4>
<p>Generative AI can help sidestep sensitive data breaches by mass-producing synthetic datasets—such as medical or financial data—for training and analysis. Such datasets contain new data points that mirror the statistical relationships found in the original data but will have no associations to real entities. Therefore, it can be safely used for a variety of purposes without compromising privacy. </p>

<div class="break">
<p class="break1">By leveraging synthetic data, organisations can mitigate the risks associated with handling sensitive information while still being able to derive valuable insights.</p>
</div>
<p>AI-generated synthetic data is already being deployed in a wide range of sectors, including healthcare and finance. For example, a health insurance company, Anthem, is currently working with Google Cloud to create a synthetic data platform that will generate approximately 1.5 to 2 petabytes of synthetic data on patients’ medical histories and healthcare claims. This synthetic data will be used to improve the training of other AI algorithms to better spot fraudulent claims and abnormalities in health records—all while keeping patients’ personal information private. </p>

<p>Nevertheless, care is still needed to ensure that synthetic data does not inadvertently reveal private data and that extreme outliers are not identifiable. By leveraging synthetic data, organisations can mitigate the risks associated with handling sensitive information while still being able to derive valuable insights and make informed decisions.</p>






<br>
<div class="containerbox">

<h3>Lessons from a Cautionary Case</h3>

<p>Responsible AI practices that build trust among stakeholders can help organisations to reap the benefits of AI. In contrast, failure to deploy algorithms responsibly may create harm, halt adoption, or even topple a government. </p>

<p>In 2021, then-Netherlands Prime Minister Mark Rutte and his entire cabinet resigned following the fallout from a flawed self-learning algorithm designed to identify childcare benefits fraud. </p>

<p>When implemented by the Tax and Customs Administration of the Netherlands, the algorithm led to tens of thousands of innocent families being accused of fraud. The accusations disproportionately targeted ethnic minorities and families with dual nationality. Families were also tagged for minor oversights such as missing signatures. Those flagged had their benefits suspended and were often forced to repay thousands of euros, plunging many into hardship and distress. The fraud investigations also had knock-on effects on their ability to access other services.</p>

<p>Aside from the reputational devastation to the government, the flawed algorithm also shook the public’s trust in the use of AI in government. Headlines like “This Algorithm Could Ruin Your Life” and “Dutch Scandal Serves as a Warning for Europe over Risks of Using Algorithms” emerged. This scandal is often used in warnings about ‘algocracy’— rule by algorithms. </p>

<p>The Netherlands’ experience highlights key questions that government leaders should ask teams in charge of AI projects:</p> 
<ol>
<li><b>What steps have you taken to ensure the quality of your data, and is your dataset sufficiently representative?</b><p></p> 
<p>In the Dutch case, officials trained the algorithm on 30,000 applications with inaccurate labelling. The files tagged as 'correct' mainly consisted of outdated applications, while the files tagged as 'incorrect' were sourced from a Tax Authority blacklist of 270,000 individuals who were categorised as potential fraudsters without evidence. The individuals on the blacklist were unaware they were on it and had no means to contest their inclusion. This flawed data labelling compromised the algorithm's integrity, resulting in inaccurate predictions.</p></li><p></p>

<li><b>What features are unacceptable, and how are you auditing the AI model to ensure continued fidelity?</b><p></p> 
<p>A feature may be prohibited because it is a socially unacceptable or even illegal consideration for a decision. In the case of the Dutch tax authority, one legally? prohibited feature was ethnicity. As AIs are very good at reconstructing features that we may forbid it to use, more frequent and careful audits could have caught the inappropriate use of discriminatory inputs.<sup><a href="#notestop">3</a></sup></p> 

</li><li><b>What trade-off is the algorithm making, and are we comfortable with it?</b><p></p> 
<p>Algorithms generally have to trade-off between recall (the ability of the algorithm to identify all relevant cases) and precision (the percentage of all identified cases that are correct).</p>

<figure>  
<img src="/images/Ethos_Images/Ethos_Digital_Issue_13/D13_AI_algorithm.png">

</figure> 

<p>The acceptable balance between recall and precision depends on policy context. High recall (and poor precision) may be acceptable if the consequences of a false positive are not severe. For instance, an algorithm predicting potential exposure to a disease might mistakenly label someone as high risk, but the consequence might only involve taking a test to confirm or rule out infection. However, in the Dutch case, an algorithm that prioritises high recall may correctly identify all fraudulent tax cases, but also include many innocent people, with severe and politically unpalatable consequences.</p></li>

<li><b>Who is responsible and accountable for the algorithm’s decision-making?</b><p></p> 
<p>In the Dutch case, officers treated the algorithm’s predictions as verdicts rather than recommendations. There was no subsequent assessment by human tax officials. The decision-making responsibility was devolved to AI, but not the ultimate accountability, as the Rutte government learned the hard way. Leaders would do well to clearly designate and communicate the responsibility and accountability for AI output and its subsequent use.</p></li>

<li><b>What ethical principles should apply here, and are we deploying such an ethical AI system?</b><p></p> 
<p>For an enforcement model meant to detect fraud, the principle of human autonomy may not be as relevant as for service delivery with personal data. However, there are other ethical principles to consider. For instance, IMDA’s Model AI Governance Framework<sup><a href="#notestop">4</a></sup> states that AI solutions should be explainable,<sup><a href="#notestop">5</a></sup>  transparent,<sup><a href="#notestop">6</a></sup> and fair.<sup><a href="#notestop">7</a></sup>  

</p><p>In this case, the Dutch tax authority did not explain to citizens why they had been targeted for fraud investigations. Additionally, its system was opaque: people did not know that they had been subject to an algorithmic decision and found it difficult to challenge the decision. The system’s flaws emerged only because of investigative reporting and parliamentary inquiries. Finally, the system was deeply unfair since it unjustifiably discriminated against certain segments of the population.</p></li>
<p></p>

</ol></div>

<h3>Safety First, to Go Faster and Further</h3>
<p>History shows us that safety features promote technology adoption. Modern cars come fully enclosed in steel with seat belts, automatic lights, and emergency braking. Additional regulations concerning speed limits and licensing further enhance driver and passenger safety. Higher risk technologies even include eject or abort features.</p>

<p>In the same vein, AI’s potential to transform government can only occur if it is adopted responsibly. Doing so will ensure the well-being of citizens, uphold ethical standards, and build trust in the beneficial capabilities of AI-driven governance. </p>

<br>

<div class="author">
<h6>ABOUT THE AUTHORS</h6>	 
	<p><b>Chelsea Ong</b> was Researcher in the Institute of Governance and Policy, Civil Service College, Singapore.</p>

<p><b>Dr Vernie Oliveiro</b> is Principal Researcher in the Institute of Governance and Policy, Civil Service College, Singapore..</p>
	
</div>

<div class="notestop" id="notestop">
<h6>NOTES</h6>
<ol>
	
<li>The World Health Organisation put forward key principles in 2021 for the governance of healthcare data systems. They include autonomy – humans, not machines, should make final decisions; safety – AI tools should be continuously monitored to ensure that they are not causing harm; and equity – AI should not be biased against certain groups of people. Governance becomes critical to ensure the success of AI adoption without potential harm.</li>
	
<li>The assessment was independently conducted by Business for Social Responsibility (BSR), a global non-profit organization. BSR uncovered some potential risks in the deployment of the model, including users over-relying on the outcomes produced, or misusing it for purposes other than what it was intended to do. </li>

<li>At the same time, because AIs can reconstruct inappropriate inputs, agencies should still collect relevant data in order to check that their AI systems have not unintentionally factored in forbidden inputs in their predictions. That is, we won’t be able to tell if an AI is discriminating against racial minorities, unless we are able to check that they are not doing so. </li>

<li>In addition to IMDA’s Model AI Governance Framework, leaders looking to implement AI solutions in the public sector can refer to GovTech’s Public Sector AI Playbook, which offers an overview of AI technology; a description of common applications; and guidance on using central AI products, developing internal capabilities, or procuring solutions.</li>

<li>While AI is often likened to a "black box", explainability pertains to our capacity to offer rationale for specific outcomes, rather than to detail the inner workings of AI's decision-making processes. In the same vein, when we are asked to explain our decisions, we provide justifications for our choices, not a description of how our synapses interact with neurotransmitters. For more on the legal and technical dimensions of explanation, see Finale Doshi-Velez, Mason Kortz, et al., “Accountability of AI Under the Law: The Role of Explanation”, https://arxiv.org/abs/1711.01134.</li>

<li>According to the OECD, transparency in AI involves, first, “disclosing when AI is being used”, and second, “enabling people to understand how an AI system is developed, trained, operates, and deployed in the relevant application domain”. Transparency heightens the chance that errors and misuse will be spotted. See OECD.AI Policy Observatory, “Transparency and explainability”, https://oecd.ai/en/dashboards/ai-principles/P7. </li>

<li>Fairness in AI is “the absence of prejudice or preference for an individual or group based on their characteristics”.  Bias can emerge from different sources. For example, historical bias can be found in data that reflects existing societal disadvantages, representation bias stems from underrepresentation of small groups in data samples, and measurement bias occurs when an input into an algorithm does not accurately reflect the real world. See Mary Reagan, “Understanding Bias and Fairness in AI Systems”, 25 March 2021, https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3.</li> 


</ol>	
</div>




<br>
<br>	
<div class="back">
<a href="/ethos/">Back to Ethos Page</a>	
</div>